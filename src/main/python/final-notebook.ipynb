{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cceb058-20d0-4047-93be-b1d9f4537d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING NON-OPTIMIZED SHUFFLES ---\n",
      "Non-Optimized Pipeline Time: 5.62 seconds\n",
      "\n",
      "--- STARTING OPTIMIZED SHUFFLES ---\n",
      "Optimized Pipeline Time: 6.57 seconds\n",
      "\n",
      "--- PHYSICAL EXECUTION PLAN (NON-OPTIMIZED) ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [risk_factor#129 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(risk_factor#129 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=708]\n",
      "      +- Project [anatom_site_general#19, sex#21, avg_age#119, total_melanoma#121, total_cases#123L, (total_melanoma#121 / cast(total_cases#123L as double)) AS risk_factor#129]\n",
      "         +- HashAggregate(keys=[anatom_site_general#19, sex#21], functions=[avg(age_approx#18), sum(MEL#45), count(image#17)])\n",
      "            +- Exchange hashpartitioning(anatom_site_general#19, sex#21, 200), ENSURE_REQUIREMENTS, [plan_id=704]\n",
      "               +- HashAggregate(keys=[anatom_site_general#19, sex#21], functions=[partial_avg(age_approx#18), partial_sum(MEL#45), partial_count(image#17)])\n",
      "                  +- Project [image#17, age_approx#18, anatom_site_general#19, sex#21, MEL#45]\n",
      "                     +- BroadcastHashJoin [image#17], [image#44], Inner, BuildRight, false\n",
      "                        :- Project [image#17, age_approx#18, anatom_site_general#19, sex#21]\n",
      "                        :  +- Generate explode([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]), [image#17, age_approx#18, anatom_site_general#19, sex#21], false, [dummy#65]\n",
      "                        :     +- Filter isnotnull(image#17)\n",
      "                        :        +- FileScan csv [image#17,age_approx#18,anatom_site_general#19,sex#21] Batched: false, DataFilters: [isnotnull(image#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://skin-milan/raw-data/ISIC_2019_Training_Metadata.csv], PartitionFilters: [], PushedFilters: [IsNotNull(image)], ReadSchema: struct<image:string,age_approx:int,anatom_site_general:string,sex:string>\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=699]\n",
      "                           +- Filter isnotnull(image#44)\n",
      "                              +- FileScan csv [image#44,MEL#45] Batched: false, DataFilters: [isnotnull(image#44)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://skin-milan/raw-data/ISIC_2019_Training_GroundTruth.csv], PartitionFilters: [], PushedFilters: [IsNotNull(image)], ReadSchema: struct<image:string,MEL:double>\n",
      "\n",
      "\n",
      "\n",
      "--- PHYSICAL EXECUTION PLAN (OPTIMIZED) ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [risk_factor#249 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(risk_factor#249 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=768]\n",
      "      +- Project [anatom_site_general#197, sex#199, avg_age#239, total_melanoma#241, total_cases#243L, (total_melanoma#241 / cast(total_cases#243L as double)) AS risk_factor#249]\n",
      "         +- HashAggregate(keys=[anatom_site_general#197, sex#199], functions=[avg(age_approx#196), sum(MEL#45), count(image#195)])\n",
      "            +- HashAggregate(keys=[anatom_site_general#197, sex#199], functions=[partial_avg(age_approx#196), partial_sum(MEL#45), partial_count(image#195)])\n",
      "               +- Exchange hashpartitioning(anatom_site_general#197, 200), REPARTITION_BY_COL, [plan_id=763]\n",
      "                  +- Project [image#195, age_approx#196, anatom_site_general#197, sex#199, MEL#45]\n",
      "                     +- BroadcastHashJoin [image#195], [image#44], Inner, BuildRight, false\n",
      "                        :- Filter isnotnull(image#195)\n",
      "                        :  +- FileScan parquet [image#195,age_approx#196,anatom_site_general#197,sex#199] Batched: true, DataFilters: [isnotnull(image#195)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://skin-milan/processed-data/metadata_cache.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(image)], ReadSchema: struct<image:string,age_approx:int,anatom_site_general:string,sex:string>\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=760]\n",
      "                           +- Filter isnotnull(image#44)\n",
      "                              +- FileScan csv [image#44,MEL#45] Batched: false, DataFilters: [isnotnull(image#44)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://skin-milan/raw-data/ISIC_2019_Training_GroundTruth.csv], PartitionFilters: [], PushedFilters: [IsNotNull(image)], ReadSchema: struct<image:string,MEL:double>\n",
      "\n",
      "\n",
      "\n",
      "Summary: Baseline (5.62s) vs Optimized (6.57s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# 1. Environment Setup\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages com.amazonaws:aws-java-sdk-s3:1.12.196,\"\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell\"\n",
    ")\n",
    "\n",
    "# 2. Spark Session Initialization\n",
    "# We start with a generic name; we will update the UI later for history tracking\n",
    "conf = SparkConf().setAppName('ISIC_Comparison_Study')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc).builder.getOrCreate()\n",
    "\n",
    "# 3. AWS Configuration\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "# here set the keys as environment variables or use IAM roles if running on AWS infrastructure\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n",
    "# Path Definitions\n",
    "s3_path_metadata = \"s3a://skin-milan/raw-data/ISIC_2019_Training_Metadata.csv\"\n",
    "s3_path_labels   = \"s3a://skin-milan/raw-data/ISIC_2019_Training_GroundTruth.csv\"\n",
    "s3_output_base   = \"s3a://skin-milan/final-results\"\n",
    "s3_parquet_path  = \"s3a://skin-milan/processed-data/metadata_cache.parquet\"\n",
    "\n",
    "# 4. Load Raw Data\n",
    "df_metadata = spark.read.csv(s3_path_metadata, header=True, inferSchema=True)\n",
    "df_labels = spark.read.csv(s3_path_labels, header=True, inferSchema=True)\n",
    "\n",
    "# 5. Data Synthesis (Scaling)\n",
    "df_large_metadata = df_metadata.withColumn(\"dummy\", F.explode(F.array([F.lit(i) for i in range(100)]))) \\\n",
    "    .withColumn(\"patient_record_id\", F.monotonically_increasing_id()) \\\n",
    "    .drop(\"dummy\")\n",
    "\n",
    "# --- PHASE 1: NON-OPTIMIZED VERSION ---\n",
    "sc.setJobGroup(\"Baseline\", \"Non-Optimized Shuffle Run\") # Helps in Spark UI/History\n",
    "print(\"\\n--- STARTING NON-OPTIMIZED SHUFFLES ---\")\n",
    "start_raw = time.time()\n",
    "\n",
    "df_joined_raw = df_large_metadata.join(df_labels, on=\"image\", how=\"inner\")\n",
    "\n",
    "df_final_risk_raw = df_joined_raw.groupBy(\"anatom_site_general\", \"sex\") \\\n",
    "    .agg(\n",
    "        F.avg(\"age_approx\").alias(\"avg_age\"),\n",
    "        F.sum(\"MEL\").alias(\"total_melanoma\"),\n",
    "        F.count(\"image\").alias(\"total_cases\")\n",
    "    ) \\\n",
    "    .withColumn(\"risk_factor\", F.col(\"total_melanoma\") / F.col(\"total_cases\")) \\\n",
    "    .orderBy(F.desc(\"risk_factor\"))\n",
    "\n",
    "# Save Final Non-Optimized Result to S3\n",
    "df_final_risk_raw.write.mode(\"overwrite\").csv(f\"{s3_output_base}/non_optimized_results\")\n",
    "\n",
    "raw_duration = round(time.time() - start_raw, 2)\n",
    "print(f\"Non-Optimized Pipeline Time: {raw_duration} seconds\")\n",
    "\n",
    "\n",
    "# --- PHASE 2: OPTIMIZED VERSION ---\n",
    "sc.setJobGroup(\"Optimized\", \"Broadcast and Partition Run\")\n",
    "print(\"\\n--- STARTING OPTIMIZED SHUFFLES ---\")\n",
    "\n",
    "# Step A: Optimization - Use Parquet for intermediate storage\n",
    "df_large_metadata.write.mode(\"overwrite\").parquet(s3_parquet_path)\n",
    "df_opt_metadata = spark.read.parquet(s3_parquet_path)\n",
    "\n",
    "start_opt = time.time()\n",
    "\n",
    "# Step B: Optimization - Broadcast Join\n",
    "df_joined_opt = df_opt_metadata.join(broadcast(df_labels), on=\"image\", how=\"inner\")\n",
    "\n",
    "# Step C: Optimization - Explicit Repartitioning\n",
    "df_final_risk_opt = df_joined_opt.repartition(\"anatom_site_general\") \\\n",
    "    .groupBy(\"anatom_site_general\", \"sex\") \\\n",
    "    .agg(\n",
    "        F.avg(\"age_approx\").alias(\"avg_age\"),\n",
    "        F.sum(\"MEL\").alias(\"total_melanoma\"),\n",
    "        F.count(\"image\").alias(\"total_cases\")\n",
    "    ) \\\n",
    "    .withColumn(\"risk_factor\", F.col(\"total_melanoma\") / F.col(\"total_cases\")) \\\n",
    "    .orderBy(F.desc(\"risk_factor\"))\n",
    "\n",
    "# Save Final Optimized Result to S3\n",
    "df_final_risk_opt.write.mode(\"overwrite\").csv(f\"{s3_output_base}/optimized_results\")\n",
    "\n",
    "opt_duration = round(time.time() - start_opt, 2)\n",
    "print(f\"Optimized Pipeline Time: {opt_duration} seconds\")\n",
    "\n",
    "# --- HISTORY & PLANS ---\n",
    "print(\"\\n--- PHYSICAL EXECUTION PLAN (NON-OPTIMIZED) ---\")\n",
    "df_final_risk_raw.explain()\n",
    "\n",
    "print(\"\\n--- PHYSICAL EXECUTION PLAN (OPTIMIZED) ---\")\n",
    "df_final_risk_opt.explain()\n",
    "\n",
    "print(f\"\\nSummary: Baseline ({raw_duration}s) vs Optimized ({opt_duration}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223f1a2-36ce-4883-838a-875c0464ef07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
